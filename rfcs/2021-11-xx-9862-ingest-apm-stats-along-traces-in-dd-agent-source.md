# RFC  - 2021-11-XX - Ingest APM stats in `datadog-agent` source

Datadog traces support in `datadog-agent` source was initially documented in this [RFC][trace-support-pr]. However the
Datadog `trace-agent` submit more data that juste traces. It also send statistics ("APM Stats") about running time of
each instrumented resource (i.e. a given piece of code) that are aggregated over time (based on 100% of the traces
received). Those APM stats are very important as they can highlight code hot spots and ease aggregation. Also as an
integral part of the Datadog APM product, APM stats should be handled by Vector.

[trace-support-pr]: https://github.com/vectordotdev/vector/pull/9634

## Context

Traces are new in Vector, initial support is focusing on traces coming from the Datadog Agent, the
[RFC][trace-support-pr] discussing about traces describes how those traces will be handled in Vector and highlighted the
need to support APM stats.

APM stats are encoded using protobuf, the current schema is available in the [datadog-agent][apm-stats-proto]
repository. APM stats may be computed by the tracing lib in some cases or most of the time by the trace agent, but once
they are emitted by the `trace-agent` there is no difference except a [boolean value][client-side-marker] that indicates
where it was computed. They are sent by the `trace-agent` to the same endpoint as trace payloads, only the path differ,
allowing easy discrimination between trace & APM stats payloads.

Those stats are computed by a component named [concentrator] in the trace-agent. There is a [dedicated
path][client-stats-aggregator] for APM stats that comes directly from tracing libraries. But ultimately they flow
through datadog with the same [sending code][stats-writer].

If we details a bit the actual content of a stats payload, it is an aggregate of
[`ClientGroupedStats`][client-grouped-stats-proto]:
```
	string service = 1;
	string name = 2;
	string resource = 3;
	uint32 HTTP_status_code = 4;
	string type = 5;
	string DB_type = 6; // db_type might be used in the future to help in the obfuscation step
	uint64 hits = 7; // count of all spans aggregated in the groupedstats
	uint64 errors = 8; // count of error spans aggregated in the groupedstats
	uint64 duration = 9; // total duration in nanoseconds of spans aggregated in the bucket
	bytes okSummary = 10; // ddsketch summary of ok spans latencies encoded in protobuf
	bytes errorSummary = 11; // ddsketch summary of error spans latencies encoded in protobuf
	bool synthetics = 12; // set to true on spans generated by synthetics traffic
	uint64 topLevelHits = 13; // count of top level spans aggregated in the groupedstats
```

So it is basically a group of various metrics that can be represented in Vector (Sketches are support since this
[PR][sketch-pr]). In the proto definition sketches are stored as unstructured bytes slices, but those
[fields][call-to-toproto] are filled with a [protobuf encoded ddsketch][ddsktech-proto]. Given that sketches in Vector
are also [heavily based on ddsketch][vector-sketch], APM stats sketches can be converted to/from the Vector internal
representation without incuring too much accuracy losses, but this would require significant work to implement those
conversion.

This opens two major very different paths for APM stats in Vector:

* Either the APM stats are emitted as a log, each [`ClientGroupedStats`][client-grouped-stats-proto] would be mapped to one log event.
* Or they are emitted as metrics each value from every [`ClientGroupedStats`][client-grouped-stats-proto] would be
  emitted as a metric with all upper level information store as tags.

This also raises the question of having a second (assuming that the `datadog-agent` sources accepts Datadog Agent
metrics - [RFC][dd-agent-metric-rfc]), unrelated, metric stream coming out of the `datadog-agent` source. Obviously
event ingested representing APM stats will have to be routed along with traces, and most often they will follow a
different path that other plain metrics/logs received from a core agent. Thus it is suggested to re-arrange the
`datadog-agent` source according to the following points (Additional details on what exactly are "Datadog Agents" can be
found in the trace support [RFC][trace-support-pr] and may provide relevant context for undermentionned points):

* Keep a single `datadog-agent` source and add a settings to switch between agent kind: `agent: <TYPE>` where `<TYPE>`
  could be  `core` (would support metrics & logs - we could optionnally add `logs` & `metrics` to only allow logs or
  metrics, along with `core` that would allow both), `trace` and could be extended to support `process`, `security` and
  so on.
* Another solution would be to keep one Vector source per kind of Datadog Agent an then we would have the following
  Vector sources:
  * `datadog-core`: would be the current `datadog-agent` sources, it would receive the data sent by the Datadog "core"
    Agent (the agent that collects logs & metrics). Note that renaming this source is probably optional, but
    documentation update would be required anyway.
  * `datadog-trace`: would support all data sent by the `trace-agent`
  * And so on as the support list grow:
    * `datadog-process` source for the Datadog `process-agent`
    * `datadog-security` for the Datadog security Agent
    * Etc.

[apm-stats-proto]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/pb/stats.proto
[client-side-marker]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/pb/stats.proto#L15
[concentrator]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/stats/concentrator.go
[client-stats-aggregator]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/stats/client_stats_aggregator.go#L25-L45
[stats-writer]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/writer/stats.go
[client-grouped-stats-proto]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/pb/stats.proto#L55-L70
[sketch-pr]: https://github.com/vectordotdev/vector/pull/9178
[call-to-toproto]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/stats/statsraw.go#L53-L62
[ddsktech-proto]: https://github.com/DataDog/sketches-go/blob/0829b5a6a3c9627d1faee6c690611067ee110e38/ddsketch/pb/ddsketch.proto
[vector-sketch]: https://github.com/vectordotdev/vector/blob/4db23b3/lib/vector-core/src/metrics/ddsketch.rs#L202-L213
[dd-agent-metric-rfc]: https://github.com/vectordotdev/vector/pull/9057

## Cross cutting concerns

None identified so far.

## Scope

### In scope

* Decode APM request (protobuf) received from the trace agent
* Convert those into a Vector internal representation
* Enable the passthrough use case (trace-agent -> Vector -> Datadog) in a lossless fashion

### Out of scope

* Compute APM stats in Vector, but this should be kept in mind as a valuable feature for third party traces
* Any support for other kind of traces

## Pain

* Vector has no trace support
* Datadog traces support without APM stats makes the whole APM product much less powerful.

## Proposal

### User Experience

* The Vector `datadog-agent` source configured with `agent: trace` will accept APM stats (along with traces), and emit
  Vector event (logs or metrics depending on the implementation)including all metadata as tags/fields, so filtering
  could be done later in the topology on both APM stats and traces.
* The `datadog-trace` sink will receive those event (metrics and/or log depending on the implementation) and do the
  opposite conversion, pending that expected tags will be there.
* Regarding the Datadog trace-agent config, the APM stats endpoint is the same as the trace one (`apm_config.apm_dd_url`
  config key), there will be nothing else to configure.
* API key management will be the same as it is for other Datadog sources/sinks.

### Implementation

* Import all APM stats as standard vector metrics in the new variation of the `datadog-agent` source that would support
  a new settings named `agent` (or `agent_type`, `agent_kind`, etc.) that would default to `core` (hence keeping the
  existing behaviour)
  * The existing `DatadogAgentConfig` struct will get a new field `agent: Box<dyn AgentKind>` with relevant serde wiring
    and wrap specific code behind that trait and keep a generic code in `DatadogAgentConfig` & `DatadogAgentSource`.
  * Turn each [`ClientGroupedStats`][client-grouped-stats-proto] into a one log event with all possible metadata to
    allow the lossless pass-through scenario and the same level of filtering/routing we can achieve for traces. Sketches
    would then be kepts undecoded as raw bytes, they would have to be store in a new metadata property. It would
    materialise as a string/bytes map. We could possbily store it in `Value::Bytes` field but this may really be unsafe
    (**TBC**).
* The upcoming `datadog-trace` sink would then receive incoming APM stats log events that will be reaggregated according
  to the relevant dimensions using the `Partitioner` trait to rebuild [APM stats payloads][apm-stats-proto].

## Rationale

* We should Keep valuable metrics relevant to end-user
* Dropping APM stats would cause current user to loose some insight on execution time.

## Drawbacks

None identified so far.

## Prior Art

N/A.

## Alternatives

Regarding the fact that we could ignore/drop incoming APM stats:

* Either completely drop APM stats, but this is not really an option as it would lead to user experience degradation
* Or disable sampling on the `trace-agent` side and compute APM stats in the `datadog-trace` sink, this could work but
  this is a lot for a initial implementation (it would required plain ddsketch suppor on top of the computation logic)
  and to match the accuracy of current APM stats, Vector would have to receive 100% of traces, which may not always be
  possible. But this would pave the wayfor generic APM stats computation wherever the traces come from.

Regarding the internal representation, APM stats could be represented either by a log event but as it really a set of
metrics caracterising a given excution chunk it could be extracted as several metric event rather that keeping all those
metrics in a log event. In that case, in the `datadog-traces` sink the following

* Incoming metrics will be buffered, and populate a struct matching the [APM stats base
  object][client-grouped-stats-proto], those struct will be stored in a map according to the very same kind of
  [keys][trace-stats-agg-key] used by the trace-agent.
* And every 10 seconds (this is the sending interval of the trace-agent) serializing and flushing those to Datadog. To
  account for late metrics the sink would have to keep 2 or 3 buckets in the past and delay flushing accordingly. This
  would rely on the [bucket timestamp][btime] kept by the trace agent and [stored in APM stats payload][csb-start].

[btime]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/stats/concentrator.go#L148-L159
[csb-start]: https://github.com/DataDog/datadog-agent/blob/dc2f202/pkg/trace/pb/stats.proto#L47

## Outstanding Questions

* Confirms `datadog-agent` source re-arrangement, and the introduction of a new settings to switch between agent type.
* The choice between opaque storage for APM Stats sketches (and represent APM stats as log event) and full sketches
  conversion (and represent APM stats as a set of metrics).

## Plan Of Attack

* [ ] Rework the Datadog agent source to make it more generic
* [ ] Implement APM stats decoding in a new source leveraging the aforementionned point
* [ ] Implement a `datadog-trace` sinks that re-aggregate the metrics from the preivous step into APM stats payloads

## Future Improvements

* Compute APM stats in the `datadog-trace` sink for any trace format.
* Overall all most improvement from the Datadog trace [RFC][trace-support-pr] applies here, but having constraints on
  the schema use (in the case we represent APM stats as log event) would be very useful here.

